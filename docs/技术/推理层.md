# 推理层

## 职责

推理层提供 LLM 服务抽象和 Agent 实现，负责：
- LLM 服务的统一接口
- 流式/非流式对话
- OmniAgent 核心智能体

## 使用方式

### LLM 服务

```python
from src.reasoning.llm import LlmRegistry, LlmConfig, Message, MessageRole

# 获取服务
llm = LlmRegistry.get_service("qwen")

# 配置
config = LlmConfig(
    model="qwen-turbo",
    temperature=0.7,
    max_tokens=2048,
)

# 构建消息
messages = [
    Message(role=MessageRole.SYSTEM, content="你是一个助手"),
    Message(role=MessageRole.USER, content="你好"),
]

# 流式调用
async for chunk in llm.chat_stream(messages, config):
    print(chunk.delta, end="")

# 非流式调用
response = await llm.chat(messages, config)
print(response.content)
```

### OmniAgent

OmniAgent 继承自 Qwen-Agent 的 Assistant，支持 RAG 和 Function Calling：

```python
from src.reasoning.agent import OmniAgent

agent = OmniAgent(
    llm={"model": "qwen-max"},
    system_message="你是一个智能助手",
    function_list=["code_interpreter"],
)

# 运行
for response in agent.run(messages=[{"role": "user", "content": "写个 hello world"}]):
    print(response)
```

### 注册新的 LLM 服务

```python
from src.reasoning.llm import LlmRegistry, BaseLlmService

class CustomLlmService(BaseLlmService):
    async def chat(self, messages, config):
        # 实现同步调用
        pass
    
    async def chat_stream(self, messages, config):
        # 实现流式调用
        yield LlmChunk(delta="...")

# 注册
LlmRegistry.register("custom", CustomLlmService)
```

## 注意事项

| 注意点 | 说明 |
|-------|-----|
| API Key | 从 Nacos 或环境变量 `DASHSCOPE_API_KEY` 读取 |
| 流式优先 | 生产环境建议使用流式接口提升响应速度 |
| Token 限制 | 注意 `max_tokens` 设置，避免截断 |
| 模型选择 | `qwen-turbo` 快速，`qwen-max` 效果好 |

## 可用模型

| 模型 | 特点 |
|-----|-----|
| `qwen-turbo` | 快速响应，适合简单任务 |
| `qwen-plus` | 平衡性能和效果 |
| `qwen-max` | 最佳效果，复杂任务 |
